# -*- coding: utf-8 -*-
"""modules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yuqNYv4YyEGqjDpuSBO0cWIKvHnTyH8g
"""

class PatchEmbedding(nn.Module):
    MIN_NUM_PATCHES = 4
    def __init__(self,
                 image_size=256,
                 patch_size=16,
                 embed_dim=768,
                 input_channels=3):
        super(PatchEmbedding, self).__init__()

        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = 256

        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
        # in = 3, out = 768 ,kernel = 16, stride = 16
        self.cls_token = nn.Parameter(torch.randn(1,1, embed_dim))
        self.positions = nn.Parameter(torch.randn((image_size // patch_size) **2 + 1, embed_dim))

    def forward(self, x):

        x = self.conv(x)
        b, c, h, w = x.shape #b,768,16,16
        x = torch.reshape(x, (b, c, h * w))# batch,768,16*16
        x = torch.transpose(x,  2, 1)#torch.Size([b, 256, 768])

        cls_tokens = self.cls_token.repeat(b,1,1) #([b, 1, 768])
        x = torch.cat([cls_tokens, x], dim=1)#([b, 257, 768])

        positions = torch.randn((256// 16) **2 + 1, 768)
        x += self.positions#([b, 257, 768])
        # print(x.size())
        return x

def scaled_dot_Attention(q, k, v, mask=None, dropout=None):
    d_k = q.size(-1)
    attn_logits = torch.matmul(q, k.transpose(-2, -1))/ math.sqrt(d_k)
    if mask is not None:
        attn_logits = attn_logits.masked_fill(mask == 0, -1e9)
    attention = F.softmax(attn_logits, dim=-1)
    return torch.matmul(attention, v), attention

class MultiHeadAttention(nn.Module):
    def __init__(self,
                 embed_dim=768,
                 num_heads=12,
                 dropout=0.0):
        super(MultiHeadAttention, self).__init__()

        self.K = nn.Linear(embed_dim,embed_dim)
        self.Q = nn.Linear(embed_dim,embed_dim)
        self.V = nn.Linear(embed_dim,embed_dim)
        self.num_heads = 12
        self.embed_dim=768
        self.d_k = 64
        self.lc = nn.Linear(embed_dim,embed_dim)




    def forward(self, x, mask=None):
        batch_size, seq_length, _embed_dim = x.size()#([b, 257, 768])
        key = self.K(x)
        key = rearrange( key, "b n (h d) -> b h n d", h=self.num_heads)
        query = self.Q(x)
        query = rearrange( query , "b n (h d) -> b h n d", h=self.num_heads)
        values = self.V(x)
        values = rearrange( values, "b n (h d) -> b h n d", h=self.num_heads)
        
        # (batch_size, head = 12, seq_length = 257, d = 64)
        values, attention = scaled_dot_Attention(query, key, values, mask=mask)
        # print(values.size())
        values = rearrange(values, "b h n d -> b n (h d)")
        # print(values.size())
        values = self.lc(values)
        # print(values.size())
        return values

class FeedForwardBlock(nn.Module):
    def __init__(self, emb_size=768, expansion = 4, drop_p = 0.0):
        super(FeedForwardBlock, self).__init__()
        self.layer_norm_1 = nn.LayerNorm(emb_size)
        self.attn = nn.MultiheadAttention(emb_size,num_heads=12,dropout=0.0)
        self.layer_norm_2 = nn.LayerNorm(emb_size)
        self.linear = nn.Sequential(
            nn.Linear(emb_size, expansion * emb_size),
            nn.GELU(),
            nn.Dropout(drop_p),
            nn.Linear(expansion * emb_size, emb_size),
            nn.Dropout(drop_p)
        )
        # self.lc1= nn.Linear(emb_size, expansion * emb_size)
        # self.act_1 = nn.GELU()
        # self.drop = nn.Dropout(drop_p)
        # self.lc2 = nn.Linear(expansion * emb_size, emb_size)
      
    def forward(self, x):
        inp_x = self.layer_norm_1(x)
        x = x + self.attn(inp_x, inp_x, inp_x)[0]
        x = x + self.linear(self.layer_norm_2(x))
        return x

class TransformerEncoder(nn.Sequential):
    def __init__(self, depth: int = 6, **kwargs):
        super().__init__(*[FeedForwardBlock(**kwargs) for _ in range(depth)])

class ClassificationHead(nn.Module):
    def __init__(self, emb_size: int = 768, n_classes: int = 2):
        super().__init__()
        
        self.nrom1 = nn.LayerNorm(emb_size)
        self.lc1 = nn.Linear(emb_size, n_classes)

    def forward(self, x):
        x = reduce(x,'b n e -> b e', reduction='mean')
        x = self.nrom1(x)
        x =  self.lc1(x)
        return x

class ViT(nn.Sequential):
    def __init__(self,     
                in_channels: int = 3,
                patch_size: int = 16,
                emb_size: int = 768,
                img_size: int = 256,
                depth: int = 12,
                n_classes: int = 2):
        super().__init__(
            PatchEmbedding(),
            TransformerEncoder(),
            ClassificationHead()
        )